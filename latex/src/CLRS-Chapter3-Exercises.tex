
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}




\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\small\color{blue},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\author{JÃ¶rg Barkoczi}
\title{\vspace{-2.0cm}Chapter 3 Exercise Solutions}
\date{ }

\newtheorem{theorem}{Theorem}

\newcommand{\lra}{\Leftrightarrow}

\begin{document}
\maketitle

\setcounter{section}{2}
\section{}

% redundant but for consistency
\setcounter{subsection}{0}
\subsection{}

% set style to match CLRS
\renewcommand{\thesubsubsection}{\thesubsection-\arabic{subsubsection}}
\subsubsection{}
Let $f(n)$ and $g(n)$ be asymptotically nonnegative functions. Using the basic
definition of $\Theta$-notation, prove that $max(f(n), g(n)) = \Theta (f(n) + g(n))$.
\\~

Given constants $c_1$, $c_2$ and $n_0$, a function $(m(n) = max(f(n), g(n))) \in
\Theta (f(n) + g(n))$ if and only if $ 0 \leq c_1(f(n) + g(n)) \leq m(n) \leq c_2(f(n) + g(n))$.
Since $m(n) \leq f(n) + g(n)$ we already have an upper bound to work with and can thus default
$c_2$ to just $1$.

For the lower bound we can just take the factor $min({\frac{f(n)}{f(n) + g(n)}, \frac{g(n)}{f(n) + g(n)}})$, which
shrinks the expression $f(n) + g(n)$ to $min(f(n), g(n))$ and therefore is always smaller or equal to $max(f(n), g(n))$.

And thus our function $m(n)$ satisfies $0 \leq min({\frac{f(n)}{f(n) + g(n)}, \frac{g(n)}{f(n) + g(n)}}) (f(n) + g(n))
\leq m(n) \leq f(n) + g(n), \forall n > 0$ and is indeed in the set of functions described by $\Theta(f(n) + g(n))$.

\subsubsection{}
Show that for any real constants $a$ and $b$, where $b > 0$,
\[(n+a)^b = \Theta(n^b).\]
Let us start by showing that $(n+a)^b = O(n^b)$. This requires us to find constants $c, n_0$
such that 
\[0 \leq (n+a)^b \leq cn^b, \forall n \geq n_0.\]\\
Let $c = 2^b$ 
\[\lra (n+a)^b & \leq (2n)^b\]
and thus we have $(n+a)^b \leq (2n)^b, \forall n \geq n_0 = |a|$ which implies 
\[(n+a)^b = O(n^b).\]

Next we need to show that $(n+a)^b = \Omega(n^b)$, which again 
requires us to find constants $c, n_0$ such that
\[0 \leq cn^b \leq (n+a)^b, \forall n \geq n_0.\]\\
Let $c=(\frac{1}{2})^b$.
\begin{equation*}
    \begin{split}
        \Rightarrow (n+a)^b & \geq (\frac{1}{2}n)^b\\
    \end{split}
\end{equation*}
and thus we have $(n+a)^b \geq (\frac{1}{2}n)^b, \forall n \geq n_0 = 2|a|$ which implies 
\[(n+a)^b = \Omega(n^b).\]
And therefore, by Theorem 3.1, $(n+a)^b = \Theta(n^b)$.

\subsubsection{}
Explain why the statement, ``The running time of algorithm A
is at least $O(n^2)$'', is meaningless.
~\\

Saying that the running time of algorithm A is at least $O(n^2)$ gives
no information about the worst-case running time, because ``at least''
implies the best-case input.
It gives no information on the best-case running time either, since
the $O$-notation bounds a function from the above, not from below as 
the $\Omega$-notation does. Therefore the statement is to be considered
meaningless.

\subsubsection{}
Is $2^{n+1} = O(2^n)$?
~\\~\\
Inequality to prove \[0 \leq 2^{n+1} \leq c \cdot 2^n, \forall n \geq n_0.\]
Let $c = 2$, then
\[2^{n+1} \leq 2 \cdot 2^n = 2^{n+1}, \forall n \geq 0.\]
Therefore $2^{n+1} = O(n^2)$.\\
\pagebreak
~\\
Is $2^{2n} = O(2^n)$?
~\\~\\
Inequality to prove \[0 \leq 2^{2n} \leq c \cdot 2^n, \forall n \geq n_0.\]

\begin{equation*}
    \begin{split}
        2^{2n} & \leq c \cdot 2^n\\
        2^{n} & \leq c\\
    \end{split}
\end{equation*}
There is obviously no constant $c$ that satisfies
\[\lim_{n \to \infty} 2^{n} & \leq c,\]
therefore $2^{2n} \neq O(2^n)$.

\subsubsection{}
Prove Theorem 3.1.
~\\~\\
``For any two functions $f(n)$ and $g(n)$, we have $f(n) = \Theta(g(n))$
if and only if $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$'' (Theorem 3.1).
~\\~\\
Per definition $\Theta(g(n))$ requires the existence of constants $c_1, c_2, n_0$ 
such that \[0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n), \forall n \geq n_0.\]
Therefore we can split of the inequalities as 
\[0 \leq c_1 \cdot g(n) \leq f(n), \forall n \geq n_0,\] which implies $f(n) = \Omega(n)$, and
\[0 \leq f(n) \leq c_2 \cdot g(n), \forall n \geq n_0,\] which implies $f(n) = O(n)$.\\
From the other viewpoint, if we have constants $c_a, c_b, n_a, n_b$ such that
\[0 \leq c_a \cdot g(n) \leq f(n), \forall n \geq n_a,\] and
\[0 \leq f(n) \leq c_b \cdot g(n), \forall n \geq n_b,\]
we can let $n_0 = max(n_a, n_b)$ and thus satisfy 
\[0 \leq c_a \cdot g(n) \leq f(n) \leq c_b \cdot g(n), \forall n \geq n_0.\]




\end{document}
